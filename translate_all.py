from google import genai
from google.genai import types
import cloudscraper
import requests
from bs4 import BeautifulSoup
import time
import os
import re
import random
import json
from urllib.parse import urljoin

# ==========================================
# ‚öôÔ∏è ‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤
# ==========================================
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY") 
NOVEL_MAIN_URL = "https://kakuyomu.jp/works/822139839754922306"

JSON_DB_FILE = "novels.json"
HISTORY_FILE = "history_novel_2.txt"

if GEMINI_API_KEY:
    try:
        client = genai.Client(api_key=GEMINI_API_KEY)
    except:
        client = None
else:
    client = None

scraper = cloudscraper.create_scraper(
    browser={'browser': 'chrome', 'platform': 'windows', 'desktop': True}
)

# ==========================================
# üõ†Ô∏è ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏õ‡∏• (‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà)
# ==========================================

def translate_title(text):
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏õ‡∏•‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á/‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏≠‡∏ô ‡πÉ‡∏´‡πâ‡∏™‡∏±‡πâ‡∏ô ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à"""
    if not client or not text: return text # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ Key ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°
    
    prompt = f"""
    Translate this Japanese novel title to Thai.
    Target audience: Teenagers / Light Novel readers.
    Style: Catchy, Short, Natural.
    
    Strict Rules:
    1. Output ONLY the translated text.
    2. No explanations, no notes, no options.
    
    Original Text:
    {text}
    """
    try:
        # ‡πÉ‡∏ä‡πâ Flash Model ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏ß
        response = client.models.generate_content(
            model='gemini-2.5-pro', 
            contents=prompt,
            config=types.GenerateContentConfig(safety_settings=[
                types.SafetySetting(category='HARM_CATEGORY_HARASSMENT', threshold='BLOCK_NONE'),
                types.SafetySetting(category='HARM_CATEGORY_HATE_SPEECH', threshold='BLOCK_NONE'),
                types.SafetySetting(category='HARM_CATEGORY_SEXUALLY_EXPLICIT', threshold='BLOCK_NONE'),
                types.SafetySetting(category='HARM_CATEGORY_DANGEROUS_CONTENT', threshold='BLOCK_NONE')
            ])
        )
        if response.text:
            return response.text.strip().replace("‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á:", "").replace("‡πÅ‡∏õ‡∏•:", "").strip()
        return text
    except:
        return text # ‡∏ñ‡πâ‡∏≤ Error ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô‡πÑ‡∏õ‡∏Å‡πà‡∏≠‡∏ô

# ==========================================
# üõ†Ô∏è ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ JSON
# ==========================================

def get_novel_title():
    """‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢ + ‡πÅ‡∏õ‡∏•‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ó‡∏¢"""
    print(f"üìñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å: {NOVEL_MAIN_URL}")
    try:
        response = scraper.get(NOVEL_MAIN_URL)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        title_elem = soup.select_one('#workTitle') or soup.select_one('h1')
        raw_title = title_elem.text.strip() if title_elem else "‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö‡∏ä‡∏∑‡πà‡∏≠"
        
        # üü¢ ‡∏™‡∏±‡πà‡∏á‡πÅ‡∏õ‡∏•‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
        thai_title = translate_title(raw_title)
        
        print(f"‚úÖ ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ó‡∏¢: {thai_title} (Original: {raw_title})")
        return thai_title
    except Exception as e:
        print(f"‚ùå ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e}")
        return "‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö‡∏ä‡∏∑‡πà‡∏≠"

def save_to_json(novel_title, ep_data):
    data = {}
    if os.path.exists(JSON_DB_FILE):
        with open(JSON_DB_FILE, "r", encoding="utf-8") as f:
            try:
                content = f.read()
                if content: data = json.loads(content)
                if isinstance(data, list): data = {}
            except: data = {}

    novel_id = NOVEL_MAIN_URL
    
    if novel_id not in data:
        data[novel_id] = { "title": novel_title, "chapters": [] }
    
    # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ó‡∏¢‡πÄ‡∏™‡∏°‡∏≠ (‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•)
    data[novel_id]["title"] = novel_title
    
    chapters = data[novel_id]["chapters"]
    existing_idx = next((index for (index, d) in enumerate(chapters) if d["link"] == ep_data["link"]), None)
    
    if existing_idx is not None:
        chapters[existing_idx] = ep_data
    else:
        chapters.append(ep_data)
        
    data[novel_id]["chapters"] = chapters

    with open(JSON_DB_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
        print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà {ep_data['ep_id']} ‡∏•‡∏á JSON ‡πÅ‡∏•‡πâ‡∏ß")

# ==========================================
# üõ†Ô∏è ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô Crawler (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
# ==========================================

def load_history():
    if not os.path.exists(HISTORY_FILE): return set()
    with open(HISTORY_FILE, "r", encoding="utf-8") as f: return set(l.strip() for l in f)

def save_to_history(url):
    with open(HISTORY_FILE, "a", encoding="utf-8") as f: f.write(url + "\n")

def get_first_episode_url():
    try:
        response = scraper.get(NOVEL_MAIN_URL)
        soup = BeautifulSoup(response.text, 'html.parser')
        l = soup.select_one('a#readFromFirstEpisode')
        if l: return urljoin(NOVEL_MAIN_URL, l['href'])
        ts = re.compile(r'/works/\d+/episodes/\d+')
        ls = soup.find_all('a', href=ts)
        if ls: 
            sl = sorted(ls, key=lambda x: int(re.search(r'episodes/(\d+)', x['href']).group(1)))
            return urljoin(NOVEL_MAIN_URL, sl[0]['href'])
    except: pass
    return None

def find_next_link(soup, url):
    n = soup.select_one('a.widget-episode-navigation-next') or soup.select_one('a#contentMain-readNextEpisode') or soup.find('a', string=re.compile('Ê¨°„ÅÆ„Ç®„Éî„ÇΩ„Éº„Éâ'))
    return urljoin(url, n['href']) if n else None

def get_content_and_next_link(url, max=3):
    h={'Referer': NOVEL_MAIN_URL}
    for _ in range(max):
        try:
            time.sleep(1)
            r = scraper.get(url, headers=h, timeout=15)
            if r.status_code==200:
                s = BeautifulSoup(r.text, 'html.parser')
                t = s.select_one('.widget-episodeTitle').text.strip()
                b = s.select_one('.widget-episodeBody').get_text(separator="\n", strip=True)
                eid = re.search(r'episodes/(\d+)', url).group(1)
                return {"title":t, "content":b, "next_link":find_next_link(s, url), "ep_id":eid}
        except: time.sleep(2)
    return None

def translate_smart(text, retry_count=0):
    if not client or not text: return None, "Error"
    
    # --- üõ°Ô∏è ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå 1-3: ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÅ‡∏õ‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏Å‡πâ‡∏≠‡∏ô ---
    prompts = [
        # ‡∏£‡∏≠‡∏ö 0: ‡∏õ‡∏Å‡∏ï‡∏¥
        f"‡πÅ‡∏õ‡∏•‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ó‡∏¢ ‡∏™‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏±‡∏¢‡∏£‡∏∏‡πà‡∏ô (‡πÄ‡∏Å‡πá‡∏ö‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏Ñ‡∏£‡∏ö):\n{text[:15000]}",
        # ‡∏£‡∏≠‡∏ö 1: Soften
        f"‡πÅ‡∏õ‡∏•‡πÇ‡∏î‡∏¢‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Ñ‡∏≥‡∏•‡πà‡∏≠‡πÅ‡∏´‡∏•‡∏°‡πÅ‡∏•‡∏∞‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á (Soft Version):\n{text[:15000]}",
        # ‡∏£‡∏≠‡∏ö 2: Summary
        f"‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (‡∏ï‡∏±‡∏î‡∏â‡∏≤‡∏Å‡πÄ‡∏£‡∏ó‡∏ó‡∏¥‡πâ‡∏á ‡πÄ‡∏•‡πà‡∏≤‡πÅ‡∏Ñ‡πà‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå):\n{text[:15000]}"
    ]

    if retry_count < 3:
        try:
            prompt = prompts[retry_count]
            if retry_count > 0: print(f"   üîß ‡πÅ‡∏Å‡πâ‡πÄ‡∏Å‡∏°‡∏£‡∏≠‡∏ö‡∏ó‡∏µ‡πà {retry_count}...")
            
            response = client.models.generate_content(
                model='gemini-2.5-pro', 
                contents=prompt,
                config=types.GenerateContentConfig(safety_settings=[
                    types.SafetySetting(category='HARM_CATEGORY_HARASSMENT', threshold='BLOCK_NONE'),
                    types.SafetySetting(category='HARM_CATEGORY_HATE_SPEECH', threshold='BLOCK_NONE'),
                    types.SafetySetting(category='HARM_CATEGORY_SEXUALLY_EXPLICIT', threshold='BLOCK_NONE'),
                    types.SafetySetting(category='HARM_CATEGORY_DANGEROUS_CONTENT', threshold='BLOCK_NONE')
                ])
            )
            if response.text and response.text.strip(): return response.text, None
        except Exception as e:
            if "429" in str(e): time.sleep(10); return translate_smart(text, retry_count)
            pass # ‡∏ñ‡πâ‡∏≤ Error ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡πÉ‡∏´‡πâ‡∏´‡∏•‡∏∏‡∏î‡πÑ‡∏õ‡∏ó‡∏≥‡∏£‡∏≠‡∏ö‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
        
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏£‡∏≠‡∏ö‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
        time.sleep(2)
        return translate_smart(text, retry_count + 1)

    # --- ‚úÇÔ∏è ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå 4 (‡πÑ‡∏°‡πâ‡∏ï‡∏≤‡∏¢): ‡∏´‡∏±‡πà‡∏ô‡∏Ñ‡∏£‡∏∂‡πà‡∏á‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏õ‡∏•‡∏ó‡∏µ‡∏•‡∏∞‡∏™‡πà‡∏ß‡∏ô ---
    if retry_count == 3:
        print("   ‚öîÔ∏è ‡πÑ‡∏°‡πâ‡∏ï‡∏≤‡∏¢‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢: ‡∏´‡∏±‡πà‡∏ô‡∏Ñ‡∏£‡∏∂‡πà‡∏á‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏õ‡∏• (Split Mode)...")
        try:
            mid = len(text) // 2
            part1 = text[:mid]
            part2 = text[mid:]
            
            # ‡πÅ‡∏õ‡∏•‡∏™‡πà‡∏ß‡∏ô‡πÅ‡∏£‡∏Å
            res1, err1 = translate_smart(part1, retry_count=1) # ‡πÉ‡∏ä‡πâ‡πÇ‡∏´‡∏°‡∏î Soften ‡πÅ‡∏õ‡∏•
            # ‡πÅ‡∏õ‡∏•‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏á
            res2, err2 = translate_smart(part2, retry_count=1) # ‡πÉ‡∏ä‡πâ‡πÇ‡∏´‡∏°‡∏î Soften ‡πÅ‡∏õ‡∏•
            
            # ‡∏£‡∏ß‡∏°‡∏£‡πà‡∏≤‡∏á
            full_text = ""
            full_text += res1 if res1 else "‚ö†Ô∏è [‡∏™‡πà‡∏ß‡∏ô‡πÅ‡∏£‡∏Å‡πÅ‡∏õ‡∏•‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô]\n"
            full_text += "\n\n--- (‡∏ï‡πà‡∏≠) ---\n\n"
            full_text += res2 if res2 else "‚ö†Ô∏è [‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô]\n"
            
            return full_text, None
        except Exception as e:
            return None, f"‡∏¢‡∏≠‡∏°‡πÅ‡∏û‡πâ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏£‡∏¥‡∏á‡πÜ ({e})"
            
    # ‡∏ñ‡πâ‡∏≤‡∏´‡∏•‡∏∏‡∏î‡∏°‡∏≤‡∏ñ‡∏∂‡∏á‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡πÑ‡∏°‡πà‡πÑ‡∏´‡∏ß‡πÅ‡∏•‡πâ‡∏ß
    fallback = "‚ö†Ô∏è ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÅ‡∏£‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏õ‡∏•‡πÑ‡∏î‡πâ (‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö)"
    return fallback, None
# ==========================================
# üöÄ Main Loop
# ==========================================

def main():
    print("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏ö‡∏ö Web Novel (‡πÅ‡∏õ‡∏•‡πÑ‡∏ó‡∏¢‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏ö‡∏ö)...")
    
    # 1. ‡πÅ‡∏õ‡∏•‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡πà‡∏≠‡∏ô
    novel_title = get_novel_title()
    
    completed_urls = load_history()
    current_url = get_first_episode_url()
    
    if not current_url: 
        print("‚ùå ‡∏´‡∏≤‡∏ï‡∏≠‡∏ô‡πÅ‡∏£‡∏Å‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠"); return

    ep_count = 1
    
    while current_url:
        print(f"\n[{ep_count}] ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö: {current_url}")
        
        if current_url in completed_urls:
            print("   ‚è© ‡∏°‡∏µ‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÅ‡∏•‡πâ‡∏ß -> ‡∏Ç‡πâ‡∏≤‡∏°")
            data = get_content_and_next_link(current_url) 
            if data and data['next_link']:
                current_url = data['next_link']
                ep_count += 1
                continue
            else:
                break

        data = get_content_and_next_link(current_url)
        if not data: break

        # üü¢ ‡∏™‡∏±‡πà‡∏á‡πÅ‡∏õ‡∏•‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏≠‡∏ô‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
        print(f"   ‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏≠‡∏ô: {data['title']}")
        thai_chapter_title = translate_title(data['title'])
        
        print("   ‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤...")
        translated_content, err = translate_smart(data['content'])
        
        if translated_content:
            print(f"   ‚úÖ ‡πÅ‡∏õ‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à -> ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å")
            
            ep_data = {
                "ep_id": data['ep_id'],
                "title": thai_chapter_title, # ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡πÅ‡∏•‡πâ‡∏ß
                "content": translated_content,
                "link": current_url
            }
            
            save_to_json(novel_title, ep_data)
            save_to_history(current_url)
            completed_urls.add(current_url)
        else:
            print(f"   ‚ùå ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô: {err}")

        if data['next_link']:
            print("   ‚û°Ô∏è ‡πÑ‡∏õ‡∏ï‡∏≠‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ...")
            current_url = data['next_link']
            ep_count += 1
            time.sleep(5)
        else:
            print("üèÅ ‡∏à‡∏ö‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á")
            break

if __name__ == "__main__":
    main()
