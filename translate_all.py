from google import genai
from google.genai import types
import cloudscraper
import requests
from bs4 import BeautifulSoup
import time
import os
import re
import random
import json
from urllib.parse import urljoin

# ==========================================
# ‚öôÔ∏è ‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤
# ==========================================
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY") 
NOVEL_MAIN_URL = "https://kakuyomu.jp/works/822139839754922306"

JSON_DB_FILE = "novels.json"
HISTORY_FILE = "history_novel_2.txt"

if GEMINI_API_KEY:
    try:
        client = genai.Client(api_key=GEMINI_API_KEY)
    except:
        client = None
else:
    client = None

scraper = cloudscraper.create_scraper(
    browser={'browser': 'chrome', 'platform': 'windows', 'desktop': True}
)

# ==========================================
# üõ†Ô∏è ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏õ‡∏• (‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà)
# ==========================================

def translate_title(text):
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏õ‡∏•‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á/‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏≠‡∏ô ‡πÉ‡∏´‡πâ‡∏™‡∏±‡πâ‡∏ô ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à"""
    if not client or not text: return text # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ Key ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°
    
    prompt = f"""
    ‡πÅ‡∏õ‡∏•‡∏ä‡∏∑‡πà‡∏≠‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏≠‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢:
    - ‡∏Ç‡∏≠‡∏™‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏•‡∏ó‡πå‡πÇ‡∏ô‡πÄ‡∏ß‡∏• ‡∏ß‡∏±‡∏¢‡∏£‡∏∏‡πà‡∏ô ‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à
    - ‡∏™‡∏±‡πâ‡∏ô ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡πÑ‡∏î‡πâ‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°
    
    ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö:
    {text}
    """
    try:
        # ‡πÉ‡∏ä‡πâ Flash Model ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏ß
        response = client.models.generate_content(
            model='gemini-1.5-flash', 
            contents=prompt,
            config=types.GenerateContentConfig(safety_settings=[
                types.SafetySetting(category='HARM_CATEGORY_HARASSMENT', threshold='BLOCK_NONE'),
                types.SafetySetting(category='HARM_CATEGORY_HATE_SPEECH', threshold='BLOCK_NONE'),
                types.SafetySetting(category='HARM_CATEGORY_SEXUALLY_EXPLICIT', threshold='BLOCK_NONE'),
                types.SafetySetting(category='HARM_CATEGORY_DANGEROUS_CONTENT', threshold='BLOCK_NONE')
            ])
        )
        if response.text:
            return response.text.strip().replace("‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á:", "").replace("‡πÅ‡∏õ‡∏•:", "").strip()
        return text
    except:
        return text # ‡∏ñ‡πâ‡∏≤ Error ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô‡πÑ‡∏õ‡∏Å‡πà‡∏≠‡∏ô

# ==========================================
# üõ†Ô∏è ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ JSON
# ==========================================

def get_novel_title():
    """‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢ + ‡πÅ‡∏õ‡∏•‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ó‡∏¢"""
    print(f"üìñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å: {NOVEL_MAIN_URL}")
    try:
        response = scraper.get(NOVEL_MAIN_URL)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        title_elem = soup.select_one('#workTitle') or soup.select_one('h1')
        raw_title = title_elem.text.strip() if title_elem else "‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö‡∏ä‡∏∑‡πà‡∏≠"
        
        # üü¢ ‡∏™‡∏±‡πà‡∏á‡πÅ‡∏õ‡∏•‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
        thai_title = translate_title(raw_title)
        
        print(f"‚úÖ ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ó‡∏¢: {thai_title} (Original: {raw_title})")
        return thai_title
    except Exception as e:
        print(f"‚ùå ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e}")
        return "‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö‡∏ä‡∏∑‡πà‡∏≠"

def save_to_json(novel_title, ep_data):
    data = {}
    if os.path.exists(JSON_DB_FILE):
        with open(JSON_DB_FILE, "r", encoding="utf-8") as f:
            try:
                content = f.read()
                if content: data = json.loads(content)
                if isinstance(data, list): data = {}
            except: data = {}

    novel_id = NOVEL_MAIN_URL
    
    if novel_id not in data:
        data[novel_id] = { "title": novel_title, "chapters": [] }
    
    # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ó‡∏¢‡πÄ‡∏™‡∏°‡∏≠ (‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•)
    data[novel_id]["title"] = novel_title
    
    chapters = data[novel_id]["chapters"]
    existing_idx = next((index for (index, d) in enumerate(chapters) if d["link"] == ep_data["link"]), None)
    
    if existing_idx is not None:
        chapters[existing_idx] = ep_data
    else:
        chapters.append(ep_data)
        
    data[novel_id]["chapters"] = chapters

    with open(JSON_DB_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
        print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà {ep_data['ep_id']} ‡∏•‡∏á JSON ‡πÅ‡∏•‡πâ‡∏ß")

# ==========================================
# üõ†Ô∏è ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô Crawler (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
# ==========================================

def load_history():
    if not os.path.exists(HISTORY_FILE): return set()
    with open(HISTORY_FILE, "r", encoding="utf-8") as f: return set(l.strip() for l in f)

def save_to_history(url):
    with open(HISTORY_FILE, "a", encoding="utf-8") as f: f.write(url + "\n")

def get_first_episode_url():
    try:
        response = scraper.get(NOVEL_MAIN_URL)
        soup = BeautifulSoup(response.text, 'html.parser')
        l = soup.select_one('a#readFromFirstEpisode')
        if l: return urljoin(NOVEL_MAIN_URL, l['href'])
        ts = re.compile(r'/works/\d+/episodes/\d+')
        ls = soup.find_all('a', href=ts)
        if ls: 
            sl = sorted(ls, key=lambda x: int(re.search(r'episodes/(\d+)', x['href']).group(1)))
            return urljoin(NOVEL_MAIN_URL, sl[0]['href'])
    except: pass
    return None

def find_next_link(soup, url):
    n = soup.select_one('a.widget-episode-navigation-next') or soup.select_one('a#contentMain-readNextEpisode') or soup.find('a', string=re.compile('Ê¨°„ÅÆ„Ç®„Éî„ÇΩ„Éº„Éâ'))
    return urljoin(url, n['href']) if n else None

def get_content_and_next_link(url, max=3):
    h={'Referer': NOVEL_MAIN_URL}
    for _ in range(max):
        try:
            time.sleep(1)
            r = scraper.get(url, headers=h, timeout=15)
            if r.status_code==200:
                s = BeautifulSoup(r.text, 'html.parser')
                t = s.select_one('.widget-episodeTitle').text.strip()
                b = s.select_one('.widget-episodeBody').get_text(separator="\n", strip=True)
                eid = re.search(r'episodes/(\d+)', url).group(1)
                return {"title":t, "content":b, "next_link":find_next_link(s, url), "ep_id":eid}
        except: time.sleep(2)
    return None

def translate_smart(text, r=0):
    if not client or not text: return None, "Error"
    # Prompt ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
    ps = [
        f"‡πÅ‡∏õ‡∏•‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ó‡∏¢ ‡∏™‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏±‡∏¢‡∏£‡∏∏‡πà‡∏ô:\n‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤:\n{text[:15000]}",
        f"**‡πÅ‡∏õ‡∏•‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏•‡πà‡∏≠‡πÅ‡∏´‡∏•‡∏°**:\n‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤:\n{text[:15000]}",
        f"‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á:\n‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤:\n{text[:15000]}"
    ]
    try:
        res = client.models.generate_content(
            model='gemini-2.5-pro', contents=ps[min(r,2)],
            config=types.GenerateContentConfig(safety_settings=[
                types.SafetySetting(category='HARM_CATEGORY_HARASSMENT', threshold='BLOCK_NONE'),
                types.SafetySetting(category='HARM_CATEGORY_HATE_SPEECH', threshold='BLOCK_NONE'),
                types.SafetySetting(category='HARM_CATEGORY_SEXUALLY_EXPLICIT', threshold='BLOCK_NONE'),
                types.SafetySetting(category='HARM_CATEGORY_DANGEROUS_CONTENT', threshold='BLOCK_NONE')
            ])
        )
        if not res.text: raise ValueError("Empty")
        return res.text, None
    except Exception as e:
        if r<2: time.sleep(2); return translate_smart(text, r+1)
        return None, str(e)

# ==========================================
# üöÄ Main Loop
# ==========================================

def main():
    print("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏ö‡∏ö Web Novel (‡πÅ‡∏õ‡∏•‡πÑ‡∏ó‡∏¢‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏ö‡∏ö)...")
    
    # 1. ‡πÅ‡∏õ‡∏•‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡πà‡∏≠‡∏ô
    novel_title = get_novel_title()
    
    completed_urls = load_history()
    current_url = get_first_episode_url()
    
    if not current_url: 
        print("‚ùå ‡∏´‡∏≤‡∏ï‡∏≠‡∏ô‡πÅ‡∏£‡∏Å‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠"); return

    ep_count = 1
    
    while current_url:
        print(f"\n[{ep_count}] ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö: {current_url}")
        
        if current_url in completed_urls:
            print("   ‚è© ‡∏°‡∏µ‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÅ‡∏•‡πâ‡∏ß -> ‡∏Ç‡πâ‡∏≤‡∏°")
            data = get_content_and_next_link(current_url) 
            if data and data['next_link']:
                current_url = data['next_link']
                ep_count += 1
                continue
            else:
                break

        data = get_content_and_next_link(current_url)
        if not data: break

        # üü¢ ‡∏™‡∏±‡πà‡∏á‡πÅ‡∏õ‡∏•‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏≠‡∏ô‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
        print(f"   ‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏≠‡∏ô: {data['title']}")
        thai_chapter_title = translate_title(data['title'])
        
        print("   ‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤...")
        translated_content, err = translate_smart(data['content'])
        
        if translated_content:
            print(f"   ‚úÖ ‡πÅ‡∏õ‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à -> ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å")
            
            ep_data = {
                "ep_id": data['ep_id'],
                "title": thai_chapter_title, # ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡πÅ‡∏•‡πâ‡∏ß
                "content": translated_content,
                "link": current_url
            }
            
            save_to_json(novel_title, ep_data)
            save_to_history(current_url)
            completed_urls.add(current_url)
        else:
            print(f"   ‚ùå ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô: {err}")

        if data['next_link']:
            print("   ‚û°Ô∏è ‡πÑ‡∏õ‡∏ï‡∏≠‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ...")
            current_url = data['next_link']
            ep_count += 1
            time.sleep(5)
        else:
            print("üèÅ ‡∏à‡∏ö‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á")
            break

if __name__ == "__main__":
    main()
